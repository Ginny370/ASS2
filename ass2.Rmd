---
title: "ASS2"
author: "JingyiPei_u7457361"
date: "2022-10-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

GitHub link: https://github.com/Ginny370/ASS2

```{r}
library(tidyverse)
library(metafor)
library(orchaRd)
library(flextable)
```

# Statistical Analysis and Interpretation

## 1. Correct analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv) to generate the summary statistics (means, SD, N) for each of the fish species’ average activity for each treatment.

```{r}
# load data
d_OA <- read.csv("./data/OA_activitydat_20190302_BIOL3207.csv")
# move out NA and generate the summary statistic
data <-d_OA %>% group_by(species, treatment) %>%
              summarise(mean = mean(activity, na.rm = TRUE),
                        sd = sd(activity, na.rm = TRUE),
                        n = length(unique(animal_id)), .groups = "drop") %>%
              rename(Species = "species")
```

## 2. Through coding, merge the summary statistics generated from 1) with the metadata (i.e., clark_paper_data.csv) from Clark et al. (2020).

```{r}
# load clark paper data
c_meta <- read.csv("./data/clark_paper_data.csv")
# combine data
sum1 <- cbind(c_meta, data)
```

## 3. Through coding, correctly merge the combined summary statistics and metadata from Clark et al. (2020) (output from 1 & 2) into the larger meta-analysis dataset (i.e., ocean_meta_data.csv).

```{r}
# load ocean_meta_data
ocean <- read.csv("./data/ocean_meta_data.csv")

# combine data (output from 1&2), add some other column to put our statistic in there
sum2 <- pivot_wider(sum1, names_from = treatment,
                     names_glue = "{treatment}_{.value}",
                     values_from = c("mean", "sd", "n"))
# because here is a little different between Clark data and ocean data, we should edit something
 ## Do some renaming of colnames
sum3 <- sum2 %>% rename("oa.mean" = CO2_mean,
                            "oa.sd" = CO2_sd,
                            "oa.n" = CO2_n,
                            "ctrl.mean" = control_mean,
                            "ctrl.sd" = control_sd,
                            "ctrl.n" = control_n)

# Reorder col names based on names in ocean
sum3 <- sum3[names(ocean)]

# Check columns are in same order
colnames(ocean) == colnames(sum3)

# Bind the two dataframes
final_meta <- rbind(ocean, sum3)
```

## 4. Correctly calculate the log response ratio (lnRR) effect size for every row of the dataframe using metafor’s escalc() function.

```{r}
# calculate log response ratio
lnRR <- metafor::escalc(measure = "ROM", m1i = ctrl.mean, m2i = oa.mean, sd1i = ctrl.sd, sd2i = oa.sd, n1i = ctrl.n, n2i = oa.n, data = final_meta,
    var.names = c("lnRR", "V_lnRR"))
```

## 5. Correct meta-analytic model fitted to the data that controls for the sampling variance of lnRR. The model should include a random effect of study and observation. Use metafor’s rma.mv() function.

```{r}
# add a column that called residual
final <- lnRR %>% mutate(residual = 1:n())
# set up a model
MLMA <- metafor::rma.mv(yi=lnRR,V=V_lnRR, random = list(~1|Study, ~1|residual), method = "REML", test = "t", dfs = "contain", data=final)
MLMA
```

## 6. Written paragraph of the findings and what they mean which is supported with a figure. The paragraph should include:
### Correct presentation and interpretation of overall meta-analytic mean and measures of uncertainty around the mean estimate (e.g., 95% confidence intervals).
```{r}
# want to convert the overall meta-analytic mean back to the correlation coefficient 
predict(MLMA, transf = "transf.ztor")
```
We can extract the 95% confidence intervals which range from -0.350 to 0.098. In other words, 95% of the time we would expect the true mean activity to fall between lnRR values of -0.350 to 0.098. It indicates that if we were to repeat the experiment many times, 95% of the confidence intervals constructed would contain the true meta-analytic mean between -0.350 to 0.098.

And in this test, our p-value=0.2656, which means in each study, there is no significant difference of mean activity between control and treatment group.

### Measures of heterogeneity in effect size estimates across studies (i.e., I2 and/or prediction intervals - see predict() function in metafor)
```{r}
## Calculate I2
i2_vals <- orchaRd::i2_ml(MLMA)

## Make a pretty table. First, lets clean up the names of the different I2
## estimates. Lets remove I2_. It's a string, so, we can use some regular
## expressions to fix that. `gsub` is pretty useful. You put a pattern in and
## tell it what you would like to replace the text with. In this case, just
## blank will do! Then, we'll make the first letter of what is left
## capitalised.
i2 <- tibble(type = firstup(gsub("I2_", "", names(i2_vals))), I2 = i2_vals)


# Make a pretty table. 
flextable(i2) %>%
    align(part = "header", align = "center") %>%
    compose(part = "header", j = 1, value = as_paragraph(as_b("Type"))) %>%
    compose(part = "header", j = 2, value = as_paragraph(as_b("I"), as_b(as_sup("2")),
        as_b("(%)")))
``` 

the flextable shows that we have highly heterogeneous effect size data.

From the multilevel meta-analytic model we find that only 10.55% of the total variation in effect size estimates is the result of differences between studies.

### Forest plot showing the mean estimate, 95% confidence interval, and prediction interval with clearly labelled axes, number of samples and studies plotted on figure

```{r}
#try to use "Life.Stage" to exam the lnRR
MLMR <- metafor::rma.mv(lnRR ~ Life.stage, V = V_lnRR, method = "REML", random = list( ~1 | Study, ~1 | residual), dfs = "contain", test = "t", data = final)
MLMR
```

```{r, Figure1}
# Make an orchard plot using the model object, Orchard plot showing the mean Zr for correlation coefficients estimated between
orchaRd::orchard_plot(MLMR, mod = "Life.stage", group = "Study", data = final,xlab = "log response ratio (lnRR) effect", angle = 45)
```
Figure1. Orchard plot showing the mean lnRR for correlation coefficients estimated among life stage. It does not shows that the life stage will affect the effect size.

## 7.  plot for visually assessing the possibility of publication bias.
```{r, Figure2}
metafor::funnel(x = final$lnRR, vi = final$V_lnRR, yaxis = "seinv",
    digits = 2, xlim=c(-2,2), ylim=c(1,125),level = c(0.1, 0.05, 0.01), shade = c("white", "cyan1", "cyan4", "darkblue"),col="orange",
    las = 1, xlab = "log response ratio (lnRR) effect", atransf = tanh, legend = TRUE)
```
Figure2. Funnel plot depicting the correlation between metabolism and fitness as a function of precision (1 / SE).

## 8. Time-lag plot assessing how effect sizes may or may not have changed through time.

```{r, Figure3}
ggplot(final, aes(y = lnRR, x = Year..print., size = 1/sqrt(V_lnRR))) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Publication Year",
    y = "log response ratio (lnRR) effect", size = "Precision (1/SE)") +
    theme_classic()
```
Figure3. Plot of lnRR as a function of publication year.

## 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias

```{r}
metareg_time <- rma.mv(lnRR ~ Year..print., V = V_lnRR, random = list(~1 | Study, ~1 | residual),
    test = "t", dfs = "contain", data = final)
summary(metareg_time)
```

```{r}
r2_time <- orchaRd::r2_ml(metareg_time)
```

## 10. Formal meta-regression model that includes inverse sampling variance (i.e., 1vlnRR) to test for file-drawer biases

```{r}
lvlnRR <- rma.mv(lnRR ~ (1 / V_lnRR), V = V_lnRR, random = list(~1 | Study, ~1 | residual), test = "t", dfs = "contain", data = final)
summary(lvlnRR)
```
## 11. A written paragraph that discusses the potential for publication bias based on the meta-regression results. What type of publication bias, if any, appears to be present in the data? If publication bias is present, what does it mean and what might be contributing to such bias?

Figure2 shows that there exists a publication bias.And there is a funnel asymmetry, showing a bunch of missing effect sizes in the bottom right corner of the funnel. In other words there is a negative correlation between BMR and fitness.

In order to why will publicaion exist, we would like to assume that it would be a time-lag bias.Figure3 shows that there is a positive relationship with year and difference between the mean activity of treatment. The earlier year studies have much higher sampling variance (i.e., lower precision), just like we might expect.

It means that the time-log may contribute to the publication bias.

# 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

We play around with meta analysis and I think my our updated meta-analysis results compared with a meta-analysis by Clement et. al. (2022) has some similar point.

And maybe we still need more analysis to figure out what is the actual reason conducting to our publication bias.
